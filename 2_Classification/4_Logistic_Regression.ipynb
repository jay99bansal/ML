{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classification Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood at step 0: -14.79505628460326\n",
      "Predictions: [1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.weights = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def log_likelihood(self, X, y, weights):\n",
    "        scores = np.dot(X, weights)\n",
    "        ll = np.sum(y * scores - np.log(1 + np.exp(scores)))\n",
    "        return ll\n",
    "\n",
    "    def gradient_ascent(self, X, y):\n",
    "        # Add intercept term to X\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        X = np.hstack((intercept, X))\n",
    "        \n",
    "        # Initialize weights\n",
    "        weights = np.zeros(X.shape[1])\n",
    "        \n",
    "        # Iterate to perform gradient ascent\n",
    "        for step in range(self.max_iter):\n",
    "            predictions = self.sigmoid(np.dot(X, weights))\n",
    "            # Update weights with gradient\n",
    "            output_error_signal = y - predictions\n",
    "            gradient = np.dot(X.T, output_error_signal)\n",
    "            weights += self.learning_rate * gradient\n",
    "            \n",
    "            # Print log-likelihood every so often\n",
    "            if step % 1000 == 0:\n",
    "                print(f'Log-Likelihood at step {step}: {self.log_likelihood(X, y, weights)}')\n",
    "        \n",
    "        return weights\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.weights = self.gradient_ascent(X, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        X = np.hstack((intercept, X))\n",
    "        return self.sigmoid(np.dot(X, self.weights))\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities >= 0.5).astype(int)\n",
    "\n",
    "# Example usage\n",
    "# Generate some synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 3)\n",
    "y = (np.dot(X, [1, 2, 3]) + 1 > 0).astype(int)  # Generate labels based on a decision boundary\n",
    "\n",
    "# Create and train the model\n",
    "model = LogisticRegression(learning_rate=0.1, max_iter=500)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict new data\n",
    "test_X = np.random.randn(5, 3)\n",
    "predictions = model.predict(test_X)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood at step 0: -2.9110657652278604\n",
      "Log-Likelihood at step 1000: -2.958654627539962\n",
      "Log-Likelihood at step 2000: -2.5919252182851102\n",
      "Log-Likelihood at step 3000: -2.2450666349707267\n",
      "Log-Likelihood at step 4000: -1.9270877539226579\n",
      "Log-Likelihood at step 5000: -1.6870048674928937\n",
      "Log-Likelihood at step 6000: -1.5779142953468905\n",
      "Log-Likelihood at step 7000: -1.4828159810510675\n",
      "Log-Likelihood at step 8000: -1.3993452682221021\n",
      "Log-Likelihood at step 9000: -1.325600079401551\n",
      "Prediction for income 65: Purchase\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[40], [55], [30], [70]])  # Income of each person\n",
    "y = np.array([0, 1, 0, 1])  # Purchase outcome, 0 = no, 1 = yes\n",
    "\n",
    "# Step 2: Create and train the logistic regression model\n",
    "model = LogisticRegression(learning_rate=0.001, max_iter=10000)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Step 3: Make a prediction for an income of 65\n",
    "test_X = np.array([[65]])  # The income value for which we want to predict\n",
    "prediction = model.predict(test_X)\n",
    "\n",
    "# Output the prediction\n",
    "print(\"Prediction for income 65:\", \"Purchase\" if prediction[0] == 1 else \"No Purchase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at step 0: 0.6927271279971412\n",
      "Cost at step 100: 0.6870496120496606\n",
      "Cost at step 200: 0.6877542401811778\n",
      "Cost at step 300: 0.6878748376265419\n",
      "Cost at step 400: 0.687892936847626\n",
      "Cost at step 500: 0.6878956706047441\n",
      "Cost at step 600: 0.6878960892186421\n",
      "Cost at step 700: 0.687896153941847\n",
      "Cost at step 800: 0.6878961640099109\n",
      "Cost at step 900: 0.6878961655819738\n",
      "Predictions: [0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0\n",
      " 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1\n",
      " 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RegularizedLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, regularization='l2', C=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.regularization = regularization\n",
    "        self.C = C  # Regularization strength (smaller values specify stronger regularization)\n",
    "        self.weights = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def compute_cost(self, X, y, weights):\n",
    "        \"\"\"Compute the cost function, optionally including L1 or L2 regularization.\"\"\"\n",
    "        m = X.shape[0]\n",
    "        predictions = self.sigmoid(np.dot(X, weights))\n",
    "        log_loss = - np.sum(y * np.log(predictions + 1e-9) + (1 - y) * np.log(1 - predictions + 1e-9)) / m\n",
    "        \n",
    "        if self.regularization == 'l2':\n",
    "            l2_penalty = (self.C / 2) * np.sum(np.square(weights[1:]))\n",
    "            return log_loss + l2_penalty\n",
    "        elif self.regularization == 'l1':\n",
    "            l1_penalty = self.C * np.sum(np.abs(weights[1:]))\n",
    "            return log_loss + l1_penalty\n",
    "        else:\n",
    "            return log_loss\n",
    "\n",
    "    def gradient_descent(self, X, y):\n",
    "        \"\"\"Perform gradient descent, including L1 or L2 regularization adjustments.\"\"\"\n",
    "        m, n = X.shape\n",
    "        weights = np.zeros(n)\n",
    "        for step in range(self.max_iter):\n",
    "            predictions = self.sigmoid(np.dot(X, weights))\n",
    "            errors = predictions - y\n",
    "            gradient = np.dot(X.T, errors) / m\n",
    "            \n",
    "            if self.regularization == 'l2':\n",
    "                # Exclude bias term from regularization\n",
    "                gradient[1:] += self.C * weights[1:] / m\n",
    "            elif self.regularization == 'l1':\n",
    "                # Compute subgradient for L1\n",
    "                gradient[1:] += self.C * np.sign(weights[1:]) / m\n",
    "            \n",
    "            weights -= self.learning_rate * gradient\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                cost = self.compute_cost(X, y, weights)\n",
    "                print(f\"Cost at step {step}: {cost}\")\n",
    "                \n",
    "        return weights\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the logistic regression model to the data.\"\"\"\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        X = np.hstack((intercept, X))\n",
    "        self.weights = self.gradient_descent(X, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Calculate the probability of the classes.\"\"\"\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        X = np.hstack((intercept, X))\n",
    "        return self.sigmoid(np.dot(X, self.weights))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for samples in X.\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities >= 0.5).astype(int)\n",
    "\n",
    "# Example usage\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 3)  # 100 samples, 3 features\n",
    "y = np.random.randint(0, 2, size=100)  # Binary targets\n",
    "\n",
    "model = RegularizedLogisticRegression(learning_rate=0.1, max_iter=1000, regularization='l2', C=0.1)\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "print(\"Predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
