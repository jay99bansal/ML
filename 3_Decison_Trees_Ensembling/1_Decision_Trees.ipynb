{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-Down Approach (Categorical Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "Outlook (IG: 0.2467)\n",
      "    Sunny:\n",
      "    Humidity (IG: 0.9710)\n",
      "        High:\n",
      "        Prediction: No\n",
      "        Normal:\n",
      "        Prediction: Yes\n",
      "    Overcast:\n",
      "    Prediction: Yes\n",
      "    Rain:\n",
      "    Windy (IG: 0.9710)\n",
      "        False:\n",
      "        Prediction: Yes\n",
      "        True:\n",
      "        Prediction: No\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],\n",
    "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
    "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
    "    'Windy': ['False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'True'],\n",
    "    'Play Tennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to calculate entropy\n",
    "def entropy(target):\n",
    "    total = len(target)\n",
    "    value_counts = Counter(target)\n",
    "    return -sum((count / total) * np.log2(count / total) for count in value_counts.values() if count > 0)\n",
    "\n",
    "# Function to calculate Information Gain\n",
    "def information_gain(data, feature, target):\n",
    "    total_entropy = entropy(data[target])\n",
    "    weighted_entropy = 0\n",
    "    \n",
    "    for value in data[feature].unique():\n",
    "        subset = data[data[feature] == value]\n",
    "        weighted_entropy += (len(subset) / len(data)) * entropy(subset[target])\n",
    "        \n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "# Class for Tree Node\n",
    "class TreeNode:\n",
    "    def __init__(self, feature=None, value=None, children=None, prediction=None, ig=None):\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.children = children if children is not None else {}\n",
    "        self.prediction = prediction\n",
    "        self.ig = ig  # Store information gain for this node\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.prediction is not None\n",
    "\n",
    "# Function to build the decision tree\n",
    "def build_tree(data, target):\n",
    "    # If all target values are the same, return a leaf node\n",
    "    if len(data[target].unique()) == 1:\n",
    "        return TreeNode(prediction=data[target].values[0])\n",
    "\n",
    "    best_gain = 0\n",
    "    best_feature = None\n",
    "\n",
    "    # Determine the best feature to split on using Information Gain\n",
    "    for feature in data.columns[:-1]:  # Exclude target\n",
    "        gain = information_gain(data, feature, target)\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_feature = feature\n",
    "\n",
    "    # Create a node for the best feature\n",
    "    tree_node = TreeNode(feature=best_feature, ig=best_gain)\n",
    "\n",
    "    # Split the dataset on the best feature\n",
    "    for value in data[best_feature].unique():\n",
    "        subset = data[data[best_feature] == value]\n",
    "        # Recursively build the tree for the subset\n",
    "        tree_node.children[value] = build_tree(subset, target)\n",
    "\n",
    "    return tree_node\n",
    "\n",
    "# Function to display the tree with feature names and Information Gain\n",
    "def display_tree(node, level=0):\n",
    "    indent = \" \" * (level * 4)  # Indentation based on level\n",
    "    if node.is_leaf():\n",
    "        print(f\"{indent}Prediction: {node.prediction}\")\n",
    "    else:\n",
    "        print(f\"{indent}{node.feature} (IG: {node.ig:.4f})\")  # Display feature and IG\n",
    "        for value in node.children:\n",
    "            print(f\"{indent}    {value}:\")\n",
    "            display_tree(node.children[value], level + 1)\n",
    "\n",
    "# Build the decision tree\n",
    "decision_tree = build_tree(df, 'Play Tennis')\n",
    "\n",
    "# Display the decision tree\n",
    "print(\"Decision Tree:\")\n",
    "display_tree(decision_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-Down Approach (Numerical Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Rules:\n",
      "|--- Weight (kg) <= 18.50\n",
      "|   |--- class: No\n",
      "|--- Weight (kg) >  18.50\n",
      "|   |--- class: Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "# Sample dataset without Animal ID\n",
    "data = {\n",
    "    'Weight (kg)': [22, 25, 47, 15, 40, 10, 5, 35],\n",
    "    'Class': ['Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target\n",
    "X = df[['Weight (kg)']]  # Input feature(s)\n",
    "y = df['Class']          # Target variable\n",
    "\n",
    "# Create and train the decision tree classifier\n",
    "clf = DecisionTreeClassifier(criterion='entropy')  # You can use 'gini' or 'entropy'\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Display the decision tree\n",
    "tree_rules = export_text(clf, feature_names=['Weight (kg)'])\n",
    "print(\"Decision Tree Rules:\")\n",
    "print(tree_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART Algorithm for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "Type (Gini Gain: 0.4688)\n",
      "    Mammal:\n",
      "    Prediction: Yes\n",
      "    Reptile:\n",
      "    Prediction: No\n",
      "    Fish:\n",
      "    Prediction: No\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Sample dataset without Animal ID\n",
    "data = {\n",
    "    'Color': ['Brown', 'Black', 'White', 'Black', 'Green', 'Brown', 'Green', 'White'],\n",
    "    'Size': ['Small', 'Small', 'Large', 'Large', 'Small', 'Medium', 'Large', 'Medium'],\n",
    "    'Type': ['Mammal', 'Mammal', 'Mammal', 'Reptile', 'Reptile', 'Mammal', 'Fish', 'Mammal'],\n",
    "    'Class': ['Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'No', 'Yes']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to calculate Gini Index\n",
    "def gini_index(target):\n",
    "    total = len(target)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    value_counts = Counter(target)\n",
    "    return 1 - sum((count / total) ** 2 for count in value_counts.values())\n",
    "\n",
    "# Function to calculate Gini Gain\n",
    "def gini_gain(data, feature, target):\n",
    "    total_gini = gini_index(data[target])\n",
    "    weighted_gini = 0\n",
    "    \n",
    "    for value in data[feature].unique():\n",
    "        subset = data[data[feature] == value]\n",
    "        weighted_gini += (len(subset) / len(data)) * gini_index(subset[target])\n",
    "        \n",
    "    return total_gini - weighted_gini\n",
    "\n",
    "# Class for Tree Node\n",
    "class TreeNode:\n",
    "    def __init__(self, feature=None, value=None, children=None, prediction=None):\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.children = children if children is not None else {}\n",
    "        self.prediction = prediction\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.prediction is not None\n",
    "\n",
    "# Function to build the decision tree using CART algorithm\n",
    "def build_tree(data, target):\n",
    "    # If all target values are the same, return a leaf node\n",
    "    if len(data[target].unique()) == 1:\n",
    "        return TreeNode(prediction=data[target].values[0])\n",
    "\n",
    "    best_gain = 0\n",
    "    best_feature = None\n",
    "\n",
    "    # Determine the best feature to split on using Gini Gain\n",
    "    for feature in data.columns[:-1]:  # Exclude target\n",
    "        gain = gini_gain(data, feature, target)\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_feature = feature\n",
    "\n",
    "    # Create a node for the best feature\n",
    "    tree_node = TreeNode(feature=best_feature)\n",
    "\n",
    "    # Split the dataset on the best feature\n",
    "    for value in data[best_feature].unique():\n",
    "        subset = data[data[best_feature] == value]\n",
    "        # Recursively build the tree for the subset\n",
    "        tree_node.children[value] = build_tree(subset, target)\n",
    "\n",
    "    return tree_node\n",
    "\n",
    "# Function to display the tree with feature names and Gini values\n",
    "def display_tree(node, level=0):\n",
    "    indent = \" \" * (level * 4)  # Indentation based on level\n",
    "    if node.is_leaf():\n",
    "        print(f\"{indent}Prediction: {node.prediction}\")\n",
    "    else:\n",
    "        gini_value = gini_gain(df, node.feature, 'Class')  # Calculate Gini gain for the current split\n",
    "        print(f\"{indent}{node.feature} (Gini Gain: {gini_value:.4f})\")\n",
    "        for value in node.children:\n",
    "            print(f\"{indent}    {value}:\")\n",
    "            display_tree(node.children[value], level + 1)\n",
    "\n",
    "# Build the decision tree\n",
    "decision_tree = build_tree(df, 'Class')\n",
    "\n",
    "# Display the decision tree\n",
    "print(\"Decision Tree:\")\n",
    "display_tree(decision_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cart Algorithm for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Tree:\n",
      "Size (sq ft)\n",
      "    1500:\n",
      "    Prediction: 300000\n",
      "    2000:\n",
      "    Prediction: 400000\n",
      "    2500:\n",
      "    Prediction: 450000\n",
      "    1800:\n",
      "    Prediction: 350000\n",
      "    2200:\n",
      "    Prediction: 410000\n",
      "    1300:\n",
      "    Prediction: 250000\n",
      "    1700:\n",
      "    Prediction: 320000\n",
      "    1600:\n",
      "    Prediction: 310000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Size (sq ft)': [1500, 2000, 2500, 1800, 2200, 1300, 1700, 1600],\n",
    "    'Rooms': [3, 4, 4, 3, 4, 2, 3, 3],\n",
    "    'Price (in $)': [300000, 400000, 450000, 350000, 410000, 250000, 320000, 310000]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to calculate Mean Squared Error (MSE)\n",
    "def mse(target):\n",
    "    if len(target) == 0:\n",
    "        return 0\n",
    "    mean_value = np.mean(target)\n",
    "    return np.mean((target - mean_value) ** 2)\n",
    "\n",
    "# Function to calculate MSE for a split\n",
    "def mse_split(data, feature, target):\n",
    "    total_mse = mse(data[target])\n",
    "    weighted_mse = 0\n",
    "    \n",
    "    for value in data[feature].unique():\n",
    "        subset = data[data[feature] == value]\n",
    "        weighted_mse += (len(subset) / len(data)) * mse(subset[target])\n",
    "        \n",
    "    return total_mse - weighted_mse\n",
    "\n",
    "# Class for Tree Node\n",
    "class TreeNode:\n",
    "    def __init__(self, feature=None, value=None, children=None, prediction=None):\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.children = children if children is not None else {}\n",
    "        self.prediction = prediction\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.prediction is not None\n",
    "\n",
    "# Function to build the regression tree\n",
    "def build_regression_tree(data, target):\n",
    "    # If all target values are the same, return a leaf node\n",
    "    if len(data[target].unique()) == 1:\n",
    "        return TreeNode(prediction=data[target].values[0])\n",
    "\n",
    "    best_mse_gain = 0\n",
    "    best_feature = None\n",
    "\n",
    "    # Determine the best feature to split on using MSE\n",
    "    for feature in data.columns[:-1]:  # Exclude target\n",
    "        gain = mse_split(data, feature, target)\n",
    "        if gain > best_mse_gain:\n",
    "            best_mse_gain = gain\n",
    "            best_feature = feature\n",
    "\n",
    "    # Create a node for the best feature\n",
    "    tree_node = TreeNode(feature=best_feature)\n",
    "\n",
    "    # Split the dataset on the best feature\n",
    "    for value in data[best_feature].unique():\n",
    "        subset = data[data[best_feature] == value]\n",
    "        # Recursively build the tree for the subset\n",
    "        tree_node.children[value] = build_regression_tree(subset, target)\n",
    "\n",
    "    return tree_node\n",
    "\n",
    "# Function to display the regression tree\n",
    "def display_regression_tree(node, level=0):\n",
    "    indent = \" \" * (level * 4)  # Indentation based on level\n",
    "    if node.is_leaf():\n",
    "        print(f\"{indent}Prediction: {node.prediction}\")\n",
    "    else:\n",
    "        print(f\"{indent}{node.feature}\")\n",
    "        for value in node.children:\n",
    "            print(f\"{indent}    {value}:\")\n",
    "            display_regression_tree(node.children[value], level + 1)\n",
    "\n",
    "# Build the regression tree\n",
    "regression_tree = build_regression_tree(df, 'Price (in $)')\n",
    "\n",
    "# Display the regression tree\n",
    "print(\"Regression Tree:\")\n",
    "display_regression_tree(regression_tree)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
